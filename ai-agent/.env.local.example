# Local LLM Configuration (for agent_local.py)

# Choose your local model
# Options: llama-3.3-70b-instruct, codellama-13b-instruct, llama-3-sqlcoder-8b
LOCAL_MODEL=llama-3.3-70b-instruct

# Ollama API base URL (default: http://localhost:11434)
LOCAL_LLM_BASE_URL=http://localhost:11434

# MCP Server Configuration
MCP_SERVER_URL=http://localhost:8001/mcp

# Agent Configuration
MAX_ITERATIONS=5
TEMPERATURE=0.7
MAX_TOKENS=2000
